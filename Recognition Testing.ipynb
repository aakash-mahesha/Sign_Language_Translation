{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f602c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3140ae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path=os.path.join(os.getcwd(),os.path.join('Datasets','custom_dataset.csv'))\n",
    "df=pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a21783cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'A'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=pd.unique(df['value'])\n",
    "word_dict={index: value for index, value in enumerate(words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf75ac99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_10 (Dropout)         (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 40)                1720      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 30)                1230      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 31        \n",
      "=================================================================\n",
      "Total params: 2,981\n",
      "Trainable params: 2,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_path=os.path.join(os.getcwd(),os.path.join('saved_models','custom_model.h5'))\n",
    "model=keras.models.load_model(model_path)\n",
    "print('Loaded custome Neural Net')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3a739c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "cap = cv2.VideoCapture(0,cv2.CAP_DSHOW)\n",
    "rows=[]\n",
    "with mp_hands.Hands(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    max_num_hands=1) as hands:\n",
    "    while True:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "              # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "        image=cv2.flip(image,1)\n",
    "        cv2.putText(image, \"Testing \", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "                \n",
    "            # Flip the image horizontally for a later selfie-view display, and convert\n",
    "            # the BGR image to RGB.\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                # To improve performance, optionally mark the image as not writeable to\n",
    "                # pass by reference.\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        results = hands.process(image)\n",
    "\n",
    "                # Draw the hand annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            row=[]\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                for hand_landmark in hand_landmarks.landmark:\n",
    "                    row.append(hand_landmark.x)\n",
    "                    row.append(hand_landmark.y)\n",
    "                            \n",
    "                \n",
    "                if len(row)<42:\n",
    "                    break\n",
    "                row=np.array([row])\n",
    "                prediction=np.argmax(model.predict(row))\n",
    "                if prediction not in word_dict.keys():\n",
    "                    break\n",
    "                cv2.putText(image,word_dict[prediction],(400,80),cv2.FONT_HERSHEY_SIMPLEX,0.9,(0,255,0),2)\n",
    "        cv2.imshow('MediaPipe Hands', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3543ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
